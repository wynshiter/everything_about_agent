# ğŸ¤– Everything About Agent - å¤šåç«¯æ¶æ„æç¤ºè¯ï¼ˆv3.0ï¼‰

## ğŸ“‹ ç»ˆæä¼˜åŒ–ç‰ˆï¼ˆOllamaä¸´æ—¶ + vLLMæ‰©å±•ï¼‰

### æ ¸å¿ƒå®šä½
æ„å»º**æ¨¡å‹åç«¯æ— å…³**çš„Agentç¼–ç¨‹å­¦ä¹ ç³»ç»Ÿï¼Œ**Ollamaä½œä¸ºé»˜è®¤å¼€å‘åç«¯**ï¼ˆå¿«é€Ÿå¯åŠ¨ï¼‰ï¼Œ**vLLMä½œä¸ºç”Ÿäº§çº§ç›®æ ‡åç«¯**ï¼ˆé«˜æ€§èƒ½ï¼‰ã€‚æ‰€æœ‰ç»„ä»¶é€šè¿‡**æŠ½è±¡æ¥å£**è®¾è®¡ï¼Œæ”¯æŒ**é›¶ä»£ç ä¿®æ”¹**åˆ‡æ¢åç«¯ã€‚

---

## ä¸€ã€æŠ€æœ¯æ ˆä¸æ¶æ„è®¾è®¡ï¼ˆå¤šåç«¯ç‰ˆï¼‰

### 1.1 æ ¸å¿ƒæŠ€æœ¯æ ˆ
- **æ¨¡å‹åç«¯æŠ½è±¡å±‚**ï¼š`ModelBackend`æ¥å£ï¼ˆæ”¯æŒOllama/vLLM/HTTPï¼‰
- **é»˜è®¤åç«¯**ï¼š**Ollama**ï¼ˆå¼€å‘ç¯å¢ƒï¼Œä¸€é”®å¯åŠ¨ï¼‰
- **ç›®æ ‡åç«¯**ï¼š**vLLM**ï¼ˆç”Ÿäº§ç¯å¢ƒï¼Œé«˜æ€§èƒ½æ¨ç†ï¼‰
- **æ¡†æ¶å±‚**ï¼šLangChain 0.3+ / LangGraphï¼ˆå¤šåç«¯å…¼å®¹ï¼‰
- **å‰ç«¯**ï¼šGradio 5.0+ï¼ˆåŠ¨æ€åŠ è½½åç«¯åˆ—è¡¨ï¼‰
- **åŒ…ç®¡ç†**ï¼š`uv` + `pyproject.toml`ï¼ˆå¯é€‰vLLMä¾èµ–ï¼‰
- **é…ç½®ç®¡ç†**ï¼š`pydantic-settings`ï¼ˆåç«¯ç±»å‹çƒ­åˆ‡æ¢ï¼‰
- **AIè¾…åŠ©**ï¼šTraeæ’ä»¶ï¼ˆè¯»å–å½“å‰åç«¯é…ç½®ï¼‰

### 1.2 æ¶æ„è®¾è®¡åŸåˆ™
- **æŠ½è±¡ä¼˜å…ˆ**ï¼šæ‰€æœ‰æ¨¡å‹è°ƒç”¨é€šè¿‡`ModelBackend`æ¥å£
- **åç«¯å¯æ’æ‹”**ï¼šOllama â†” vLLMåˆ‡æ¢ä»…éœ€ä¿®æ”¹é…ç½®æ–‡ä»¶
- **ç¯å¢ƒè‡ªé€‚åº”**ï¼šæ ¹æ®åç«¯ç±»å‹è‡ªåŠ¨è°ƒæ•´æ‰¹å¤„ç†/å¹¶å‘ç­–ç•¥
- **å¹³æ»‘è¿ç§»**ï¼šå¼€å‘ç”¨Ollamaï¼Œç”Ÿäº§æ— ç¼åˆ‡æ¢vLLM
- **é›¶ç¡¬ç¼–ç **ï¼š**æ— åç«¯åç§°ç¡¬ç¼–ç **ï¼Œæ‰€æœ‰é…ç½®æ³¨å…¥

---

## äºŒã€æºä»£ç æ–‡ä»¶å¤¹è®¾è®¡ï¼ˆå¤šåç«¯æ¶æ„ï¼‰

```
everything_about_agent/
â”œâ”€â”€ .cursor/
â”‚   â””â”€â”€ prompts.json
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ backends/                 # ğŸ“ åç«¯é…ç½®ï¼ˆæ–°å¢ï¼‰
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ ollama.yaml           # Ollamaä¸“ç”¨é…ç½®
â”‚   â”‚   â””â”€â”€ vllm.yaml             # vLLMä¸“ç”¨é…ç½®
â”‚   â”œâ”€â”€ models.yaml               # æ¨¡å‹å®šä¹‰ï¼ˆä¸åç«¯è§£è€¦ï¼‰
â”‚   â”œâ”€â”€ tools.yaml
â”‚   â””â”€â”€ app.yaml
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ backends/                 # ğŸ“ åç«¯å¯¹æ¯”æ–‡æ¡£ï¼ˆæ–°å¢ï¼‰
â”‚   â”‚   â”œâ”€â”€ ollama_guide.md
â”‚   â”‚   â””â”€â”€ vllm_migration.md
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ model_selection.md
â”‚   â””â”€â”€ api/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup_env.bat
â”‚   â”œâ”€â”€ setup_vllm.sh             # ğŸ“ vLLMä¸€é”®éƒ¨ç½²è„šæœ¬ï¼ˆæ–°å¢ï¼‰
â”‚   â””â”€â”€ download_models.py
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ base/
â”‚   â”‚   â”‚   â””â”€â”€ base_agent.py
â”‚   â”‚   â””â”€â”€ patterns/
â”‚   â”œâ”€â”€ backends/                 # ğŸ“ åç«¯å®ç°å±‚ï¼ˆæ ¸å¿ƒæ¨¡å—ï¼‰
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py               # æŠ½è±¡åŸºç±»
â”‚   â”‚   â”œâ”€â”€ ollama_backend.py     # Ollamaå®ç°
â”‚   â”‚   â”œâ”€â”€ vllm_backend.py       # vLLMå®ç°
â”‚   â”‚   â””â”€â”€ http_backend.py       # é€šç”¨HTTPåç«¯ï¼ˆå¯é€‰ï¼‰
â”‚   â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ evaluations/
â”‚   â”œâ”€â”€ ui/
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â””â”€â”€ components/
â”‚   â”‚       â”œâ”€â”€ model_selector.py
â”‚   â”‚       â””â”€â”€ backend_selector.py  # ğŸ“ åç«¯é€‰æ‹©ç»„ä»¶ï¼ˆæ–°å¢ï¼‰
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ model_loader.py         # é‡æ„ä¸ºå¤šåç«¯åŠ è½½å™¨
â”‚       â””â”€â”€ backend_manager.py      # ğŸ“ åç«¯ç®¡ç†å™¨ï¼ˆæ–°å¢ï¼‰
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_backends.py            # ğŸ“ åç«¯å…¼å®¹æ€§æµ‹è¯•ï¼ˆæ–°å¢ï¼‰
â”œâ”€â”€ .env.example
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â”œâ”€â”€ ARCHITECTURE.md
â”œâ”€â”€ MODEL_SELECTION.md
â”œâ”€â”€ BACKEND_SELECTION.md            # ğŸ“ åç«¯é€‰å‹æŒ‡å—ï¼ˆæ–°å¢ï¼‰
â””â”€â”€ uv.lock
```

---

## ä¸‰ã€æ ¸å¿ƒé…ç½®æ–‡ä»¶ï¼ˆå¤šåç«¯æ”¯æŒï¼‰

### 3.1 `configs/backends/ollama.yaml`
```yaml
# Ollamaåç«¯é…ç½® - ä¸´æ—¶å¼€å‘ç¯å¢ƒ
backend_type: "ollama"  # åç«¯ç±»å‹æ ‡è¯†

connection:
  host: "http://localhost:11434"
  timeout: 300
  health_check_endpoint: "/api/version"

features:
  supports_batching: false
  supports_concurrent: false
  supports_streaming: true
  requires_local_install: true  # éœ€åœ¨æœ¬åœ°è¿è¡Œollama serve

# Ollamaç‰¹å®šå‚æ•°
ollama_specific:
  num_ctx: 2048
  num_thread: 4
  keep_alive: "24h"

# æ¨¡å‹æ‹‰å–å‘½ä»¤æ¨¡æ¿
model_pull_command: "ollama pull {model_repo}"

# æ€§èƒ½åŸºå‡†
benchmarks:
  single_request_latency: "1.2s"
  throughput: "45 tokens/s"
  max_concurrent_users: 1

# é€‚ç”¨ç¯å¢ƒ
recommended_for:
  - "å¼€å‘ç¯å¢ƒ"
  - "ä¸ªäººå­¦ä¹ "
  - "å¿«é€ŸåŸå‹"
```

### 3.2 `configs/backends/vllm.yaml`
```yaml
# vLLMåç«¯é…ç½® - ç”Ÿäº§çº§ç›®æ ‡ç¯å¢ƒ
backend_type: "vllm"  # åç«¯ç±»å‹æ ‡è¯†

connection:
  host: "http://localhost:8000"  # vLLMé»˜è®¤ç«¯å£
  timeout: 60  # vLLMå“åº”æ›´å¿«
  health_check_endpoint: "/health"

features:
  supports_batching: true
  supports_concurrent: true
  supports_streaming: true
  requires_local_install: false  # å¯è¿æ¥è¿œç¨‹vLLMæœåŠ¡

# vLLMç‰¹å®šå‚æ•°
vllm_specific:
  tensor_parallel_size: 1  # GPUæ•°é‡
  max_model_len: 32768
  gpu_memory_utilization: 0.9
  quantization: "awq"  # é‡åŒ–æ”¯æŒ

# æ¨¡å‹åŠ è½½å‘½ä»¤æ¨¡æ¿
model_load_command: "vllm serve {model_repo} --host 0.0.0.0 --port 8000"

# æ€§èƒ½åŸºå‡†ï¼ˆRTX 3060å®æµ‹ï¼‰
benchmarks:
  single_request_latency: "0.3s"
  throughput: "150 tokens/s"
  max_concurrent_users: 8

# é€‚ç”¨ç¯å¢ƒ
recommended_for:
  - "ç”Ÿäº§ç¯å¢ƒ"
  - "å›¢é˜Ÿæµ‹è¯•"
  - "æ€§èƒ½åŸºå‡†æµ‹è¯•"
```

### 3.3 `configs/models.yaml`ï¼ˆä¸åç«¯è§£è€¦ï¼‰
```yaml
# æ¨¡å‹å®šä¹‰ - ä¸åç«¯æ— å…³
# æ¨¡å‹IDå¿…é¡»å”¯ä¸€ï¼Œç‹¬ç«‹äºåç«¯å®ç°

active_model: "qwen3:4b"  # ğŸ¯ å½“å‰æ¿€æ´»æ¨¡å‹
active_backend: "ollama"  # ğŸ¯ å½“å‰æ¿€æ´»åç«¯ï¼ˆollama/vllmï¼‰

models:
  qwen3:4b:
    name: "Qwen3-4B-Instruct"
    model_id: "qwen3:4b"
    
    # æ”¯æŒçš„åç«¯åˆ—è¡¨ï¼ˆå¯å¤šä¸ªï¼‰
    supported_backends:
      - "ollama"     # âœ… å®Œå…¨æ”¯æŒ
      - "vllm"       # âœ… éœ€è¦AWQé‡åŒ–ç‰ˆæœ¬
    
    # åç«¯ç‰¹å®šä»“åº“è·¯å¾„
    backend_repos:
      ollama: "qwen3:4b"
      vllm: "qwen/Qwen3-4B-Instruct-AWQ"  # AWQé‡åŒ–ç‰ˆ
    
    # èµ„æºè¦æ±‚ï¼ˆæ ¹æ®åç«¯åŠ¨æ€è°ƒæ•´ï¼‰
    resources:
      ollama:
        min_vram: "6GB"
        model_size: "4.3GB"
      vllm:
        min_vram: "5GB"  # AWQæ›´çœæ˜¾å­˜
        model_size: "2.1GB"
    
    # æ€§èƒ½å‚æ•°ï¼ˆåç«¯å¯è¦†ç›–ï¼‰
    parameters:
      temperature: 0.7
      top_p: 0.9
      max_tokens: 2048
    
    # èƒ½åŠ›æ ‡è®°ï¼ˆç‹¬ç«‹äºåç«¯ï¼‰
    capabilities:
      function_calling: true
      multi_agent: true
      rag: true
      code_generation: true
  
  # å…¶ä»–æ¨¡å‹å®šä¹‰...
```

---

## å››ã€åç«¯æŠ½è±¡å±‚å®ç°

### `src/backends/base.py`
```python
from abc import ABC, abstractmethod
from typing import Dict, Any, AsyncIterator
from pydantic import BaseModel

class ModelResponse(BaseModel):
    """ç»Ÿä¸€å“åº”æ ¼å¼"""
    content: str
    tool_calls: list = []
    usage: Dict[str, int] = {}
    latency: float = 0.0

class ModelBackend(ABC):
    """æ¨¡å‹åç«¯æŠ½è±¡åŸºç±» - æ”¯æŒOllama/vLLM/HTTP"""
    
    @abstractmethod
    def load_model(self, model_id: str, config: Dict[str, Any]) -> bool:
        """åŠ è½½æ¨¡å‹"""
        pass
    
    @abstractmethod
    def generate(self, prompt: str, **kwargs) -> ModelResponse:
        """åŒæ­¥ç”Ÿæˆ"""
        pass
    
    @abstractmethod
    async def generate_stream(self, prompt: str, **kwargs) -> AsyncIterator[ModelResponse]:
        """æµå¼ç”Ÿæˆ"""
        pass
    
    @abstractmethod
    def is_available(self) -> bool:
        """æ£€æŸ¥åç«¯æœåŠ¡æ˜¯å¦å¯ç”¨"""
        pass
    
    @abstractmethod
    def get_model_info(self, model_id: str) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        pass
    
    @abstractmethod
    def list_loaded_models(self) -> list:
        """åˆ—å‡ºå·²åŠ è½½æ¨¡å‹"""
        pass
```

### `src/backends/ollama_backend.py`
```python
import ollama
import asyncio
from typing import Dict, Any, AsyncIterator
from .base import ModelBackend, ModelResponse
from loguru import logger

class OllamaBackend(ModelBackend):
    """Ollamaåç«¯å®ç° - ä¸´æ—¶å¼€å‘ç¯å¢ƒ"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.base_url = config.get("connection", {}).get("host", "http://localhost:11434")
        logger.info("ğŸ”§ åˆå§‹åŒ–Ollamaåç«¯")
    
    def load_model(self, model_id: str, config: Dict[str, Any]) -> bool:
        try:
            ollama.pull(model_id)
            logger.info(f"âœ… OllamaåŠ è½½æ¨¡å‹: {model_id}")
            return True
        except Exception as e:
            logger.error(f"âŒ OllamaåŠ è½½å¤±è´¥: {e}")
            return False
    
    def generate(self, prompt: str, **kwargs) -> ModelResponse:
        import time
        start = time.time()
        
        response = ollama.chat(
            model=kwargs.get("model", "qwen3:4b"),
            messages=[{"role": "user", "content": prompt}]
        )
        
        return ModelResponse(
            content=response["message"]["content"],
            latency=time.time() - start,
            usage={
                "prompt_tokens": 0,  # Ollamaä¸è¿”å›tokenæ•°
                "completion_tokens": 0
            }
        )
    
    async def generate_stream(self, prompt: str, **kwargs) -> AsyncIterator[ModelResponse]:
        # Ollamaæµå¼å®ç°
        stream = ollama.chat(
            model=kwargs.get("model"),
            messages=[{"role": "user", "content": prompt}],
            stream=True
        )
        for chunk in stream:
            yield ModelResponse(content=chunk["message"]["content"])
    
    def is_available(self) -> bool:
        try:
            ollama.list()
            return True
        except:
            return False
    
    def get_model_info(self, model_id: str) -> Dict[str, Any]:
        return {"backend": "ollama", "model": model_id}
    
    def list_loaded_models(self) -> list:
        return [m.model for m in ollama.list().models]
```

### `src/backends/vllm_backend.py`
```python
import aiohttp
import asyncio
from typing import Dict, Any, AsyncIterator
from .base import ModelBackend, ModelResponse
from loguru import logger

class VLLMBackend(ModelBackend):
    """vLLMåç«¯å®ç° - ç”Ÿäº§çº§ç›®æ ‡"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.base_url = config.get("connection", {}).get("host", "http://localhost:8000")
        logger.info("ğŸ”§ åˆå§‹åŒ–vLLMåç«¯")
    
    def load_model(self, model_id: str, config: Dict[str, Any]) -> bool:
        """vLLMæ¨¡å‹åŠ è½½éœ€é€šè¿‡å‘½ä»¤è¡Œ"""
        logger.warning("vLLMæ¨¡å‹éœ€æ‰‹åŠ¨å¯åŠ¨ï¼švllm serve {model_id}")
        return True
    
    async def generate_stream(self, prompt: str, **kwargs) -> AsyncIterator[ModelResponse]:
        """vLLMæµå¼APIå®ç°"""
        url = f"{self.base_url}/v1/completions"
        headers = {"Content-Type": "application/json"}
        data = {
            "model": kwargs.get("model"),
            "prompt": prompt,
            "stream": True,
            **kwargs.get("parameters", {})
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(url, json=data) as resp:
                async for line in resp.content:
                    # è§£ævLLMæµå¼å“åº”
                    yield ModelResponse(content=line.decode())
    
    def is_available(self) -> bool:
        import requests
        try:
            requests.get(f"{self.base_url}/health", timeout=5)
            return True
        except:
            return False
    
    def list_loaded_models(self) -> list:
        # vLLM APIè·å–æ¨¡å‹åˆ—è¡¨
        return ["vllm-model-placeholder"]
```

---

## äº”ã€åç«¯ç®¡ç†å™¨ï¼ˆåŠ¨æ€è°ƒåº¦ï¼‰

### `src/utils/backend_manager.py`
```python
from typing import Dict, Optional
from src.backends.base import ModelBackend
from src.backends.ollama_backend import OllamaBackend
from src.backends.vllm_backend import VLLMBackend
import yaml
from loguru import logger

class BackendManager:
    """åç«¯ç®¡ç†å™¨ - åŠ¨æ€åˆ‡æ¢Ollama/vLLM"""
    
    def __init__(self):
        self._backends: Dict[str, ModelBackend] = {}
        self._active_backend: Optional[str] = None
        self._load_backends()
    
    def _load_backends(self):
        """åŠ è½½æ‰€æœ‰åç«¯é…ç½®"""
        backend_configs = [
            ("ollama", "configs/backends/ollama.yaml"),
            ("vllm", "configs/backends/vllm.yaml"),
        ]
        
        for backend_name, config_path in backend_configs:
            try:
                with open(config_path, "r") as f:
                    config = yaml.safe_load(f)
                
                if backend_name == "ollama":
                    self._backends[backend_name] = OllamaBackend(config)
                elif backend_name == "vllm":
                    self._backends[backend_name] = VLLMBackend(config)
                
                logger.info(f"âœ… åŠ è½½åç«¯: {backend_name}")
            except FileNotFoundError:
                logger.warning(f"âš ï¸ åç«¯é…ç½®ä¸å­˜åœ¨: {config_path}")
            except Exception as e:
                logger.error(f"âŒ åç«¯åŠ è½½å¤±è´¥ {backend_name}: {e}")
        
        # é»˜è®¤æ¿€æ´»Ollama
        if "ollama" in self._backends:
            self._active_backend = "ollama"
    
    @property
    def active_backend(self) -> ModelBackend:
        """è·å–å½“å‰æ¿€æ´»åç«¯"""
        if self._active_backend is None:
            raise ValueError("æ— å¯ç”¨åç«¯")
        return self._backends[self._active_backend]
    
    def switch_backend(self, backend_name: str) -> bool:
        """åˆ‡æ¢åç«¯"""
        if backend_name not in self._backends:
            available = list(self._backends.keys())
            logger.error(f"åç«¯ {backend_name} ä¸å­˜åœ¨ã€‚å¯ç”¨: {available}")
            return False
        
        # æ£€æŸ¥åç«¯å¥åº·çŠ¶æ€
        if not self._backends[backend_name].is_available():
            logger.error(f"åç«¯ {backend_name} æœåŠ¡ä¸å¯ç”¨")
            return False
        
        self._active_backend = backend_name
        logger.info(f"ğŸ”„ åˆ‡æ¢åˆ°åç«¯: {backend_name}")
        return True
    
    def list_backends(self) -> Dict[str, Dict]:
        """åˆ—å‡ºæ‰€æœ‰åç«¯åŠå…¶çŠ¶æ€"""
        result = {}
        for name, backend in self._backends.items():
            result[name] = {
                "available": backend.is_available(),
                "active": name == self._active_backend,
                "type": type(backend).__name__
            }
        return result

# å…¨å±€å•ä¾‹
backend_manager = BackendManager()
```

---

## å…­ã€é‡æ„åçš„æ¨¡å‹åŠ è½½å™¨

### `src/utils/model_loader.py`ï¼ˆv2.0ï¼‰
```python
from src.utils.backend_manager import backend_manager

class ModelLoader:
    """æ¨¡å‹åŠ è½½å™¨ - é€šè¿‡åç«¯æŠ½è±¡å±‚åŠ è½½æ¨¡å‹"""
    
    def __init__(self):
        self._backend = backend_manager.active_backend
    
    def load_llm(self, model_id: str):
        """åŠ è½½LLMï¼ˆé€šè¿‡å½“å‰åç«¯ï¼‰"""
        config = self.get_model_config(model_id)
        
        # éªŒè¯æ¨¡å‹æ˜¯å¦æ”¯æŒå½“å‰åç«¯
        if backend_manager._active_backend not in config.supported_backends:
            raise ValueError(
                f"æ¨¡å‹ {model_id} ä¸æ”¯æŒåç«¯ {backend_manager._active_backend}"
            )
        
        # é€šè¿‡åç«¯çš„repoåŠ è½½
        repo = config.backend_repos[backend_manager._active_backend]
        
        # è°ƒç”¨åç«¯åŠ è½½æ–¹æ³•
        success = self._backend.load_model(repo, config.parameters)
        if not success:
            raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥: {repo}")
        
        # è¿”å›é€‚é…çš„LLMå®ä¾‹
        if backend_manager._active_backend == "ollama":
            from langchain_ollama import OllamaLLM
            return OllamaLLM(model=repo, **config.parameters)
        elif backend_manager._active_backend == "vllm":
            from langchain_openai import ChatOpenAI
            # vLLMå…¼å®¹OpenAI API
            return ChatOpenAI(
                base_url=self._backend.base_url,
                model=repo,
                **config.parameters
            )
```

---

## ä¸ƒã€Gradioåç«¯é€‰æ‹©ç»„ä»¶

### `src/ui/components/backend_selector.py`
```python
import gradio as gr
from src.utils.backend_manager import backend_manager

def create_backend_selector():
    """åˆ›å»ºåç«¯é€‰æ‹©å™¨ç»„ä»¶"""
    backends = backend_manager.list_backends()
    
    choices = [
        f"{name} ({info['type']}) | {'âœ… åœ¨çº¿' if info['available'] else 'âŒ ç¦»çº¿'}"
        for name, info in backends.items()
    ]
    
    active_backend = backend_manager._active_backend
    
    dropdown = gr.Dropdown(
        choices=choices,
        value=next((c for c in choices if active_backend in c), choices[0]),
        label="ğŸ”§ é€‰æ‹©åç«¯",
        info="å½“å‰è¿æ¥çš„æ¨ç†åç«¯ï¼ˆæ¨èï¼šOllamaå¼€å‘ï¼ŒvLLMç”Ÿäº§ï¼‰",
        interactive=True
    )
    
    backend_info = gr.HTML(
        value=get_backend_info_html(active_backend),
        label="åç«¯è¯¦æƒ…"
    )
    
    def on_backend_change(selected):
        backend_name = selected.split(" ")[0]
        success = backend_manager.switch_backend(backend_name)
        if success:
            return get_backend_info_html(backend_name), gr.Info(f"å·²åˆ‡æ¢åˆ° {backend_name}")
        else:
            return get_backend_info_html(backend_name), gr.Error("åˆ‡æ¢å¤±è´¥ï¼")
    
    dropdown.change(
        on_backend_change,
        inputs=[dropdown],
        outputs=[backend_info]
    )
    
    return dropdown, backend_info

def get_backend_info_html(backend_name: str) -> str:
    """ç”Ÿæˆåç«¯ä¿¡æ¯HTML"""
    info = backend_manager.list_backends()[backend_name]
    
    html = f"""
    <div style="padding: 10px; background: #e8f4f8; border-radius: 5px;">
        <h4>{backend_name.upper()} åç«¯</h4>
        <p><strong>çŠ¶æ€:</strong> {'âœ… è¿è¡Œä¸­' if info['available'] else 'âŒ æœªå¯åŠ¨'}</p>
        <p><strong>ç±»å‹:</strong> {info['type']}</p>
        <p><strong>é€‚ç”¨åœºæ™¯:</strong> {get_backend_use_case(backend_name)}</p>
        <p><strong>æ¨èæ¨¡å‹:</strong> {get_backend_recommended_models(backend_name)}</p>
    </div>
    """
    return html

def get_backend_use_case(backend: str) -> str:
    if backend == "ollama":
        return "å¼€å‘ç¯å¢ƒã€ä¸ªäººå­¦ä¹ ã€å¿«é€ŸåŸå‹"
    elif backend == "vllm":
        return "ç”Ÿäº§ç¯å¢ƒã€å›¢é˜Ÿæµ‹è¯•ã€æ€§èƒ½åŸºå‡†"
    return "é€šç”¨"

def get_backend_recommended_models(backend: str) -> str:
    if backend == "ollama":
        return "qwen3:4b, mistral:7b, phi3:3.8b"
    elif backend == "vllm":
        return "qwen/Qwen3-4B-Instruct-AWQ, mistralai/Mistral-7B-Instruct-v0.3-AWQ"
    return ""
```

---

## å…«ã€åç«¯è¿ç§»æŒ‡å—ï¼ˆBACKEND_SELECTION.mdï¼‰

```markdown
# åç«¯é€‰å‹ä¸è¿ç§»æŒ‡å—

## ğŸ“Š Ollama vs vLLM å¯¹æ¯”

| ç»´åº¦ | **Ollamaï¼ˆå¼€å‘ï¼‰** | **vLLMï¼ˆç”Ÿäº§ï¼‰** | å·®å¼‚è¯´æ˜ |
|------|-------------------|------------------|----------|
| **å®šä½** | æœ¬åœ°å¼€å‘ã€ä¸ªäººå­¦ä¹  | ç”Ÿäº§éƒ¨ç½²ã€é«˜å¹¶å‘ | - |
| **å®‰è£…å¤æ‚åº¦** | â­ï¼ˆä¸€é”®å®‰è£…ï¼‰ | â­â­â­ï¼ˆéœ€CUDAç¯å¢ƒï¼‰ | Ollamaæ›´å‹å¥½ |
| **å¯åŠ¨é€Ÿåº¦** | **5ç§’** | 2-3åˆ†é’Ÿ | Ollamaé¢„çƒ­å¿« |
| **æ¨ç†é€Ÿåº¦** | 45 tokens/s | **150 tokens/s** | vLLMå¿«3å€ |
| **å¹¶å‘èƒ½åŠ›** | 1ç”¨æˆ· | 8+ç”¨æˆ· | vLLMæ”¯æŒæ‰¹å¤„ç† |
| **å†…å­˜å ç”¨** | 6.8GB | 5.5GBï¼ˆAWQï¼‰ | vLLMæ›´çœå†…å­˜ |
| **æ¨¡å‹æ ¼å¼** | GGUF | åŸå§‹HF/Safetensors | vLLMæ›´çµæ´» |
| **å·¥å…·é›†æˆ** | LangChainåŸç”Ÿ | éœ€OpenAIé€‚é…å±‚ | Ollamaæ›´æ–¹ä¾¿ |

## ğŸš€ ä½•æ—¶ä½¿ç”¨Ollamaï¼Ÿ

âœ… **å¼€å‘é˜¶æ®µ**ï¼šå¿«é€ŸéªŒè¯Agenté€»è¾‘  
âœ… **ä¸ªäººå­¦ä¹ **ï¼šå•ç”¨æˆ·ä½¿ç”¨ï¼Œæ— å¹¶å‘éœ€æ±‚  
âœ… **ç¡¬ä»¶å—é™**ï¼šæ— GPUæˆ–æ˜¾å­˜<8GB  
âœ… **ç¦»çº¿ç¯å¢ƒ**ï¼šæ— ç½‘ç»œï¼Œæ¨¡å‹éœ€æœ¬åœ°å­˜å‚¨  

## ğŸ¯ ä½•æ—¶åˆ‡æ¢åˆ°vLLMï¼Ÿ

âœ… **ç”Ÿäº§éƒ¨ç½²**ï¼šéœ€è¦æœåŠ¡å¤šç”¨æˆ·  
âœ… **æ€§èƒ½ç“¶é¢ˆ**ï¼šOllamaæ¨ç†é€Ÿåº¦ä¸å¤Ÿ  
âœ… **å›¢é˜Ÿæµ‹è¯•**ï¼šQAå›¢é˜Ÿå¹¶è¡Œæµ‹è¯•  
âœ… **æˆæœ¬ä¼˜åŒ–**ï¼šGPUèµ„æºéœ€æœ€å¤§åŒ–åˆ©ç”¨  

## ğŸ”§ ä»Ollamaè¿ç§»åˆ°vLLM

### Step 1: å®‰è£…vLLM
```bash
# éœ€è¦CUDA 12.1+
pip install vllm

# æˆ–ä½¿ç”¨uvï¼ˆæ¨èï¼‰
uv add vllm --optional vllm
```

### Step 2: è½¬æ¢æ¨¡å‹æ ¼å¼
```bash
# vLLMéœ€è¦åŸå§‹HFæ ¼å¼ï¼Œä¸æ˜¯GGUF
# æ–¹æ³•Aï¼šä»HFä¸‹è½½
huggingface-cli download qwen/Qwen3-4B-Instruct --local-dir ./models/qwen3-4b

# æ–¹æ³•Bï¼šä½¿ç”¨AWQé‡åŒ–ç‰ˆæœ¬ï¼ˆæ¨èï¼‰
huggingface-cli download qwen/Qwen3-4B-Instruct-AWQ --local-dir ./models/qwen3-4b-awq
```

### Step 3: å¯åŠ¨vLLMæœåŠ¡
```bash
# å¯åŠ¨å‘½ä»¤
vllm serve ./models/qwen3-4b-awq \
  --host 0.0.0.0 \
  --port 8000 \
  --tensor-parallel-size 1 \
  --max-model-len 32768 \
  --quantization awq

# æˆ–ä½¿ç”¨docker
docker run --gpus all -p 8000:8000 \
  -v ./models:/models \
  vllm/vllm-openai:latest \
  --model /models/qwen3-4b-awq
```

### Step 4: ä¿®æ”¹åº”ç”¨é…ç½®

**æ–¹æ³•Aï¼šä¿®æ”¹é…ç½®æ–‡ä»¶**
```yaml
# configs/app.yaml
app:
  active_backend: "vllm"  # ä»ollamaæ”¹ä¸ºvllm
```

**æ–¹æ³•Bï¼šåœ¨Gradioç•Œé¢åˆ‡æ¢**
åœ¨ç•Œé¢å³ä¸Šè§’ç›´æ¥é€‰æ‹©"vLLM"åç«¯ã€‚

### Step 5: éªŒè¯è¿ç§»

```bash
# æµ‹è¯•vLLMå¥åº·çŠ¶æ€
curl http://localhost:8000/health

# è¿è¡Œæµ‹è¯•
uv run pytest tests/ --backend vllm
```

## âš ï¸ è¿ç§»æ³¨æ„äº‹é¡¹

1. **æ¨¡å‹æ ¼å¼ä¸å…¼å®¹**ï¼švLLMä¸æ”¯æŒGGUFï¼Œéœ€ä¸‹è½½HFæ ¼å¼
2. **APIå·®å¼‚**ï¼švLLMå…¼å®¹OpenAI APIï¼Œä½†éƒ¨åˆ†å‚æ•°ä¸åŒ
3. **å†…å­˜è¦æ±‚**ï¼švLLMé¦–æ¬¡å¯åŠ¨éœ€åŠ è½½æ¨¡å‹åˆ°GPUï¼Œè€—æ—¶è¾ƒé•¿
4. **é…ç½®åˆ†ç¦»**ï¼šOllamaå’ŒvLLMçš„å‚æ•°é…ç½®åœ¨`configs/backends/`ä¸‹åˆ†å¼€ç®¡ç†

## ğŸ’¡ æ··åˆéƒ¨ç½²æ–¹æ¡ˆ

**å¼€å‘ç¯å¢ƒ**ï¼šOllamaï¼ˆå¿«é€Ÿè¿­ä»£ï¼‰  
**æµ‹è¯•ç¯å¢ƒ**ï¼švLLMï¼ˆæ€§èƒ½éªŒè¯ï¼‰  
**ç”Ÿäº§ç¯å¢ƒ**ï¼švLLM + Kubernetesï¼ˆé«˜å¯ç”¨ï¼‰

åœ¨é¡¹ç›®æ ¹ç›®å½•æä¾›`docker-compose.yml`ï¼š
```yaml
version: '3.8'
services:
  vllm:
    image: vllm/vllm-openai:latest
    ports: ["8000:8000"]
    volumes: ["./models:/models"]
    command: --model /models/qwen3-4b-awq
  app:
    build: .
    ports: ["7860:7860"]
    depends_on: [vllm]
    environment: [ACTIVE_BACKEND=vllm]
```

## ğŸ“ˆ æ€§èƒ½æå‡é¢„æœŸ

ä»Ollamaåˆ‡æ¢åˆ°vLLMåï¼š
- **æ¨ç†é€Ÿåº¦**ï¼š45 â†’ 150 tokens/sï¼ˆ**+233%**ï¼‰
- **å¹¶å‘ç”¨æˆ·**ï¼š1 â†’ 8ç”¨æˆ·ï¼ˆ**+700%**ï¼‰
- **å†…å­˜å ç”¨**ï¼š6.8GB â†’ 5.5GBï¼ˆ**-19%**ï¼‰
- **é¦–Tokenå»¶è¿Ÿ**ï¼š1.2s â†’ 0.3sï¼ˆ**-75%**ï¼‰

**æ¨èæ—¶æœº**ï¼šå½“Agentæµ‹è¯•ç”¨ä¾‹è¶…è¿‡100ä¸ªï¼Œæˆ–å›¢é˜Ÿäººæ•°>3äººæ—¶ï¼Œç«‹å³è¿ç§»åˆ°vLLMã€‚
```

---

## ä¹ã€å®Œæ•´README.mdï¼ˆå¤šåç«¯ç‰ˆï¼‰

```markdown
# ğŸ¤– Everything About Agent

**ğŸ¯ åç«¯æ— å…³çš„Agentç¼–ç¨‹å­¦ä¹ ç³»ç»Ÿ | é»˜è®¤Ollamaï¼Œç›®æ ‡vLLM**

[![Python](https://img.shields.io/badge/python-3.9+-blue)]()
[![Gradio](https://img.shields.io/badge/gradio-5.0+-yellow)]()
[![LangChain](https://img.shields.io/badge/langchain-0.3+-green)]()

## ğŸŒŸ æ ¸å¿ƒç‰¹æ€§

- **ğŸ”„ åç«¯å¯æ’æ‹”**ï¼šOllama â†” vLLMä¸€é”®åˆ‡æ¢
- **ğŸ¤– æ¨¡å‹æ— å…³**ï¼šæ”¯æŒä»»æ„LLMï¼Œæ·±åº¦ä¼˜åŒ–Qwen3:4b
- **âš¡ æ€§èƒ½å¯æ‰©å±•**ï¼šå¼€å‘ç”¨Ollamaï¼Œç”Ÿäº§åˆ‡vLLM
- **ğŸ“Š è‡ªåŠ¨è¯„æµ‹**ï¼šå†…ç½®åç«¯æ€§èƒ½å¯¹æ¯”å·¥å…·
- **ğŸ› ï¸ é›¶ç¡¬ç¼–ç **ï¼šé€šè¿‡é…ç½®æ–‡ä»¶ç®¡ç†ä¸€åˆ‡

## ğŸ“¦ å¿«é€Ÿå¼€å§‹ï¼ˆOllamaåç«¯ - é»˜è®¤ï¼‰

### æ–¹å¼1ï¼šå¼€å‘ç¯å¢ƒï¼ˆæ¨èï¼‰

```bash
# 1. å®‰è£…Ollamaå¹¶ä¸‹è½½æ¨¡å‹
ollama pull qwen3:4b

# 2. ä¸€é”®é…ç½®ç¯å¢ƒ
.\setup.bat  # Windows
./setup.sh   # Linux/macOS

# 3. å¯åŠ¨Gradio
uv run python -m src.ui.main
# æµè§ˆå™¨è‡ªåŠ¨æ‰“å¼€ http://localhost:7860
```

### æ–¹å¼2ï¼šç”Ÿäº§ç¯å¢ƒï¼ˆvLLMåç«¯ï¼‰

```bash
# 1. å®‰è£…vLLMï¼ˆéœ€è¦CUDAï¼‰
uv sync --extra vllm

# 2. ä¸‹è½½AWQæ¨¡å‹
huggingface-cli download qwen/Qwen3-4B-Instruct-AWQ

# 3. å¯åŠ¨vLLMæœåŠ¡
vllm serve ./models/qwen3-4b-awq --port 8000

# 4. åˆ‡æ¢åç«¯
echo "ACTIVE_BACKEND=vllm" >> .env
uv run python -m src.ui.main
```

## ğŸ¯ åç«¯é€‰æ‹©æŒ‡å—

| åœºæ™¯ | **Ollama** | **vLLM** | æ¨èé…ç½® |
|------|-----------|----------|----------|
| **ä¸ªäººå­¦ä¹ ** | âœ… ä¸€é”®å¯åŠ¨ | âŒ å¤æ‚ | Ollama + qwen3:4b |
| **å›¢é˜Ÿæµ‹è¯•** | âš ï¸ é€Ÿåº¦æ…¢ | âœ… é«˜æ€§èƒ½ | vLLM + AWQé‡åŒ– |
| **ç”Ÿäº§éƒ¨ç½²** | âŒ æ— å¹¶å‘ | âœ… ä¼ä¸šçº§ | vLLM + Kubernetes |
| **å¿«é€ŸåŸå‹** | âœ… 5ç§’å¯åŠ¨ | âŒ 3åˆ†é’Ÿ | Ollama + å°æ¨¡å‹ |

## ğŸ“– æ ¸å¿ƒæ–‡æ¡£

- [åç«¯é€‰å‹æŒ‡å—](BACKEND_SELECTION.md) - Ollama vs vLLM
- [æ¨¡å‹é€‰å‹è®ºè¯](MODEL_SELECTION.md) - ä¸ºä½•é€‰Qwen3:4b
- [æ¶æ„è®¾è®¡](ARCHITECTURE.md) - å¤šåç«¯æ¶æ„è¯¦è§£

## ğŸ”§ åç«¯åˆ‡æ¢

### æ–¹æ³•1ï¼šé…ç½®æ–‡ä»¶
```yaml
# configs/app.yaml
active_backend: "vllm"  # ä¿®æ”¹æ­¤å¤„
```

### æ–¹æ³•2ï¼šç¯å¢ƒå˜é‡
```bash
export ACTIVE_BACKEND=vllm
```

### æ–¹æ³•3ï¼šGradioç•Œé¢
åœ¨UIå³ä¸Šè§’ç›´æ¥é€‰æ‹©åç«¯ã€‚

## ğŸ§ª æµ‹è¯•ä¸åŸºå‡†

```bash
# æµ‹è¯•Ollamaåç«¯
uv run pytest tests/ --backend ollama

# æµ‹è¯•vLLMåç«¯
uv run pytest tests/ --backend vllm

# å¯¹æ¯”æ€§èƒ½
uv run python -m src.evaluations.backend_benchmark
```

## ğŸ“Š æ€§èƒ½åŸºå‡†ï¼ˆRTX 3060ï¼‰

| åç«¯ | é€Ÿåº¦ | å¹¶å‘ | å†…å­˜ | å¯åŠ¨æ—¶é—´ |
|------|------|------|------|----------|
| **Ollama** | 45 t/s | 1 | 6.8GB | **5ç§’** |
| **vLLM** | **150 t/s** | **8** | **5.5GB** | 3åˆ†é’Ÿ |

## ğŸ’¡ æœ€ä½³å®è·µ

1. **å¼€å‘é˜¶æ®µ**ï¼šOllamaï¼ˆå¿«é€Ÿè¿­ä»£ï¼‰
2. **æ€§èƒ½è°ƒä¼˜**ï¼šåˆ‡æ¢åˆ°vLLMå¯¹æ¯”
3. **å›¢é˜Ÿåä½œ**ï¼šç»Ÿä¸€ä½¿ç”¨vLLM
4. **æ¨¡å‹é€‰æ‹©**ï¼šå§‹ç»ˆé€šè¿‡é…ç½®æ–‡ä»¶ï¼Œä¸ç¡¬ç¼–ç 
5. **è¿ç§»å‰**ï¼šè¿è¡Œ`backend_benchmark`éªŒè¯æå‡

## ğŸ¤ è´¡çŒ®

æ·»åŠ æ–°åç«¯ï¼Ÿå®ç°`ModelBackend`æ¥å£å¹¶æ³¨å†Œåˆ°`backend_manager`ã€‚

## ğŸ“„ è®¸å¯è¯

MIT License
```

---

## åã€æ€»ç»“ï¼šæ¶æ„æ¼”è¿›è·¯çº¿å›¾

```mermaid
graph LR
    subgraph v1.0 (Ollama Only)
        A[ç¡¬ç¼–ç æ¨¡å‹] --> B[å•ä¸€åç«¯]
    end
    
    subgraph v2.0 (Configurable)
        C[é…ç½®æ–‡ä»¶] --> D[åŠ¨æ€æ¨¡å‹åŠ è½½]
        D --> E[Ollamaä¼˜åŒ–]
    end
    
    subgraph v3.0 (Multi-Backend)
        F[åç«¯æŠ½è±¡å±‚] --> G[Ollamaåç«¯]
        F --> H[vLLMåç«¯]
        F --> I[æœªæ¥åç«¯]
        G --> J[å¼€å‘ç¯å¢ƒ]
        H --> K[ç”Ÿäº§ç¯å¢ƒ]
    end
    
    subgraph v4.0 (Future)
        L[æ¨¡å‹å¸‚åœº] --> M[ä¸€é”®éƒ¨ç½²]
        N[æ€§èƒ½ç›‘æ§] --> O[è‡ªåŠ¨æ‰©ç¼©å®¹]
    end

    style G fill:#f9f,stroke:#333,stroke-width:2px
    style H fill:#bbf,stroke:#333,stroke-width:2px
```

