import ollama
import asyncio
from typing import Dict, Any, AsyncIterator, List
from .base import ModelBackend, ModelResponse
from loguru import logger
import time

class OllamaBackend(ModelBackend):
    """OllamaåŽç«¯å®žçŽ° - ä¸´æ—¶å¼€å‘çŽ¯å¢ƒ"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self._base_url = config.get("connection", {}).get("host", "http://localhost:11434")
        logger.info(f"ðŸ”§ åˆå§‹åŒ–OllamaåŽç«¯ at {self._base_url}")
        # Configure ollama client if needed, though the library usually defaults to localhost:11434 or OLLAMA_HOST env var
    
    def load_model(self, model_id: str, config: Dict[str, Any]) -> bool:
        try:
            logger.info(f"â¬‡ï¸ Ollamaå¼€å§‹æ‹‰å–/åŠ è½½æ¨¡åž‹: {model_id}")
            ollama.pull(model_id)
            logger.info(f"âœ… OllamaåŠ è½½æ¨¡åž‹æˆåŠŸ: {model_id}")
            return True
        except Exception as e:
            logger.error(f"âŒ OllamaåŠ è½½å¤±è´¥: {e}")
            return False
    
    def generate(self, prompt: str, **kwargs) -> ModelResponse:
        start = time.time()
        
        try:
            response = ollama.chat(
                model=kwargs.get("model", "qwen2.5:3b"),
                messages=[{"role": "user", "content": prompt}],
                options=kwargs.get("parameters", {})
            )
            
            return ModelResponse(
                content=response["message"]["content"],
                latency=time.time() - start,
                usage={
                    "prompt_tokens": response.get("prompt_eval_count", 0),
                    "completion_tokens": response.get("eval_count", 0)
                }
            )
        except Exception as e:
            logger.error(f"Ollama generation error: {e}")
            raise
    
    async def generate_stream(self, prompt: str, **kwargs) -> AsyncIterator[ModelResponse]:
        # Ollamaæµå¼å®žçŽ°
        try:
            stream = ollama.chat(
                model=kwargs.get("model", "qwen2.5:3b"),
                messages=[{"role": "user", "content": prompt}],
                stream=True,
                options=kwargs.get("parameters", {})
            )
            for chunk in stream:
                content = chunk["message"]["content"]
                if content:
                    yield ModelResponse(content=content)
        except Exception as e:
             logger.error(f"Ollama streaming error: {e}")
             raise

    def is_available(self) -> bool:
        try:
            ollama.list()
            return True
        except:
            return False
    
    def get_model_info(self, model_id: str) -> Dict[str, Any]:
        return {"backend": "ollama", "model": model_id}
    
    def list_loaded_models(self) -> List[str]:
        try:
            return [m["name"] for m in ollama.list()["models"]]
        except:
            return []
